{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to generate mutation dataset for everyday cumulative between time points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from datetime import datetime as dt\n",
    "from scipy.signal import lfilter\n",
    "import pickle as pickle\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JH Time Series Data\n",
    "\n",
    "* Time series data is cumlative per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_start = '01/23/20'\n",
    "date_up_to = '12/31/20'\n",
    "jh_path = '../COVID-19/csse_covid_19_data/csse_covid_19_time_series/'\n",
    "confirmed_global = 'time_series_covid19_confirmed_global.csv'\n",
    "deaths_global = 'time_series_covid19_deaths_global.csv'\n",
    "\n",
    "confirmed = pd.read_csv(jh_path+confirmed_global)\n",
    "deaths = pd.read_csv(jh_path+deaths_global)\n",
    "\n",
    "#Need to adjust country names to match those in CNCB\n",
    "confirmed.loc[confirmed['Country/Region']=='US','Country/Region'] = 'United States'\n",
    "confirmed.loc[confirmed['Country/Region']=='Congo (Kinshasa)','Country/Region'] = 'Democratic Republic of the Congo'\n",
    "confirmed.loc[confirmed['Country/Region']=='Korea, South','Country/Region'] = 'South Korea'\n",
    "confirmed.loc[confirmed['Country/Region']=='Czechia','Country/Region'] = 'Czech Republic'\n",
    "confirmed.loc[confirmed['Country/Region']=='Burma','Country/Region'] = 'myanmar'\n",
    "confirmed.loc[confirmed['Country/Region']=='Congo (Brazzaville)','Country/Region'] = 'republic of the congo'\n",
    "confirmed.loc[confirmed['Country/Region']==\"Cote d'Ivoire\",'Country/Region'] = 'cotedivoire'\n",
    "\n",
    "confirmed['Country/Region'] = confirmed['Country/Region'].str.lower()\n",
    "confirmed.set_index('Country/Region',inplace=True)\n",
    "\n",
    "\n",
    "deaths.loc[deaths['Country/Region']=='US','Country/Region'] = 'United States'\n",
    "deaths.loc[deaths['Country/Region']=='Congo (Kinshasa)','Country/Region'] = 'Democratic Republic of the Congo'\n",
    "deaths.loc[deaths['Country/Region']=='Korea, South','Country/Region'] = 'South Korea'\n",
    "deaths.loc[deaths['Country/Region']=='Czechia','Country/Region'] = 'Czech Republic'\n",
    "deaths.loc[deaths['Country/Region']=='Burma','Country/Region'] = 'myanmar'\n",
    "deaths.loc[deaths['Country/Region']=='Congo (Brazzaville)','Country/Region'] = 'republic of the congo'\n",
    "deaths.loc[deaths['Country/Region']==\"Cote d'Ivoire\",'Country/Region'] = 'cotedivoire'\n",
    "\n",
    "deaths['Country/Region'] = deaths['Country/Region'].str.lower()\n",
    "deaths.set_index('Country/Region',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need other data table for population number\n",
    "* doesn't matter for date as long as all the countries are covered, only getting population number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '12-31-2020.csv'\n",
    "jh_path = '../COVID-19/csse_covid_19_data/csse_covid_19_daily_reports/'\n",
    "jh_data = pd.read_csv(jh_path+date)\n",
    "\n",
    "#drop rows that have nans for incidence rate\n",
    "jh_data = jh_data[~jh_data['Incident_Rate'].isnull()]\n",
    "jh_data = jh_data[jh_data['Incident_Rate']!=0]\n",
    "\n",
    "#matching countries of jh data to that of cncb countries\n",
    "jh_data.loc[jh_data['Country_Region']=='Taiwan*','Country_Region'] = 'Taiwan'\n",
    "jh_data.loc[jh_data['Country_Region']=='US','Country_Region'] = 'United States'\n",
    "jh_data.loc[jh_data['Country_Region']=='Korea, South','Country_Region'] = 'South Korea'\n",
    "jh_data.loc[jh_data['Country_Region']=='Czechia','Country_Region'] = 'Czech Republic'\n",
    "jh_data.loc[jh_data['Country_Region']=='Burma','Country_Region'] = 'myanmar'\n",
    "jh_data.loc[jh_data['Country_Region']=='Congo (Kinshasa)','Country_Region'] = 'Democratic Republic of the Congo'\n",
    "jh_data.loc[jh_data['Country_Region']=='Congo (Brazzaville)','Country_Region'] = 'republic of the congo'\n",
    "jh_data.loc[jh_data['Country_Region']==\"Cote d'Ivoire\",'Country_Region'] = 'cotedivoire'\n",
    "\n",
    "jh_data['Population'] = np.ceil(jh_data['Confirmed']/jh_data['Incident_Rate']*100000)\n",
    "jh_data['Country_Region'] = jh_data['Country_Region'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNCB Genome Meta Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(203403, 9)\n"
     ]
    }
   ],
   "source": [
    "#cncb meta, need to match countries\n",
    "column_names = ['Virus Strain Name','Accession ID','Related ID','Nuc.Completeness','Sequence Quality','Host','Location','Sample Collection Date','Submitting Lab']\n",
    "meta = pd.read_excel('../metadata1_5.xlsx',usecols=column_names,index_col=1)\n",
    "meta.fillna(' ',inplace=True)\n",
    "\n",
    "#Remove low quality and partial reads, pretty sure cncb does not run variant annotation for these anyways\n",
    "meta = meta[meta['Sequence Quality']!='Low']\n",
    "meta = meta[meta['Nuc.Completeness']!='Partial']\n",
    "\n",
    "#set country column and lowercase\n",
    "meta['Country'] = meta['Location'].str.split('/').str[0].str.strip()\n",
    "meta['Country'] = meta['Country'].str.lower()\n",
    "\n",
    "#Adjust country typos\n",
    "meta.loc[meta['Country']=='\\u200eromania','Country'] = 'romania'\n",
    "meta.loc[meta['Country']=='viet nam','Country'] = 'vietnam'\n",
    "meta.loc[meta['Country']=='czech repubic','Country'] = 'czech republic'\n",
    "meta.loc[meta['Country']=='ivory coast','Country'] = 'cotedivoire'\n",
    "\n",
    "# #Remove Crimea and Palestine\n",
    "meta = meta[meta['Country']!='crimea']\n",
    "meta = meta[meta['Country']!='palestine']\n",
    "\n",
    "#Filter using date as well\n",
    "meta = meta[meta['Sample Collection Date']!='2020-00-00'] #bad dates in there\n",
    "meta.loc[:,'Sample Collection Date'] = pd.to_datetime(meta['Sample Collection Date'], yearfirst=True)\n",
    "dt_date_up_to = dt.strptime(date_up_to, \"%m/%d/%y\")\n",
    "dt_date_start = dt.strptime(date_start, \"%m/%d/%y\")\n",
    "meta = meta[(meta.loc[:,'Sample Collection Date']<dt_date_up_to) & (meta.loc[:,'Sample Collection Date']>dt_date_start)]\n",
    "\n",
    "print(meta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If there is a missing country in set difference, there will be nan data in output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(meta['Country'])-set(confirmed.index.str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(meta['Country']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Infections per 100k and Fatality percentage\n",
    "* Create IR/FR for all days. Cumlative up to that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "jh_daily_ir = pd.DataFrame(columns=pd.to_datetime(confirmed.columns[4::])) #start from the second date because we take difference between dates\n",
    "jh_daily_fr = pd.DataFrame(columns=pd.to_datetime(confirmed.columns[4::]))\n",
    "country_daily_cts = pd.DataFrame(columns=pd.to_datetime(confirmed.columns[4::]))\n",
    "country_death_cts = pd.DataFrame(columns=pd.to_datetime(confirmed.columns[4::]))\n",
    "\n",
    "jh_country_lvl = pd.DataFrame(columns=['cases','deaths','population'])\n",
    "\n",
    "for country in set(meta['Country'].str.lower()):\n",
    "    population = jh_data.loc[jh_data['Country_Region']==country,'Population'].sum()\n",
    "    summed_cases = confirmed.loc[confirmed.index==country,date_up_to].sum()\n",
    "    summed_deaths = deaths.loc[deaths.index==country,date_up_to].sum()\n",
    "    jh_country_lvl.loc[country] = [summed_cases,summed_deaths,population]\n",
    "    \n",
    "    cases_timeperiod = 1+confirmed.loc[confirmed.index==country].iloc[:,4::].sum().values\n",
    "    deaths_timeperiod = deaths.loc[deaths.index==country].iloc[:,4::].sum().values\n",
    "    jh_daily_ir.loc[country] = cases_timeperiod / population * 100000\n",
    "    jh_daily_fr.loc[country] = np.nan_to_num(deaths_timeperiod / cases_timeperiod) * 100\n",
    "    country_daily_cts.loc[country] = (confirmed.loc[[country]].iloc[:,4::].sum().values-confirmed.loc[[country]].iloc[:,3:-1].sum().values).clip(min=0)\n",
    "    country_death_cts.loc[country] = (deaths.loc[[country]].iloc[:,4::].sum().values-deaths.loc[[country]].iloc[:,3:-1].sum().values).clip(min=0)\n",
    "\n",
    "jh_country_lvl.sort_index(inplace=True)\n",
    "jh_daily_ir.sort_index(inplace=True)\n",
    "jh_daily_fr.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "jh_daily_ir.to_csv(f\"temporal_files/daily_counts/daily_ir_{dt.today().strftime('%m_%d_%y')}.csv\")\n",
    "jh_daily_fr.to_csv(f\"temporal_files/daily_counts/daily_fr_{dt.today().strftime('%m_%d_%y')}.csv\")\n",
    "country_daily_cts.to_csv(f\"temporal_files/daily_counts/daily_cases_{dt.today().strftime('%m_%d_%y')}.csv\")\n",
    "country_death_cts.to_csv(f\"temporal_files/daily_counts/daily_deaths_{dt.today().strftime('%m_%d_%y')}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing gff3 files\n",
    "* Change to only parse files that aren't in collection of variants. Load a saved file with collect values and which gff3 files, only update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '01_07_21'\n",
    "with open(f\"jh_cncb_daily_dates_{date}.pkl\",'rb') as file: #Contains only country name for each date\n",
    "    aggregated_mutations = pickle.load(file)\n",
    "    \n",
    "with open(f\"jh_cncb_daily_dates_identifiers_{date}.pkl\",'rb') as file:\n",
    "    processed_identifiers = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 3030/203403 [00:00<00:12, 15642.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not processed yet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 18166/203403 [00:03<01:11, 2600.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not processed yet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 20617/203403 [00:04<01:23, 2196.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not processed yet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 21497/203403 [00:05<01:27, 2088.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not processed yet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 203403/203403 [08:13<00:00, 412.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_names = os.listdir('../gff3_cncb_restructured')\n",
    "column_names = ['variant type','start','end', 'info']\n",
    "missing = []\n",
    "date_range = pd.date_range(start=jh_daily_ir.columns[0], end=jh_daily_ir.columns[-1])\n",
    "\n",
    "#Check if there is a processesd_identifier list, contains genome names already processed\n",
    "if 'processed_identifiers' not in locals():\n",
    "    processed_identifiers = []\n",
    "    \n",
    "#aggregated mutations, dictionary for each date, each sub dictionary contains a 3d list structure. [[],[],[]]\n",
    "if 'aggregated_mutations' not in locals():\n",
    "    #this is for country ir and fr for that specific day\n",
    "    aggregated_mutations = {date:{} for date in date_range}\n",
    "else:\n",
    "    for date in date_range:\n",
    "        if date not in aggregated_mutations.keys():\n",
    "            aggregated_mutations[date] = {}\n",
    "\n",
    "#for identifier in tqdm(meta[meta['Country']=='United States'].index):\n",
    "for identifier in tqdm(meta.index):\n",
    "    \n",
    "    #Use prexisting list\n",
    "    if identifier in processed_identifiers:\n",
    "        continue\n",
    "\n",
    "    #Searching for correct identifier\n",
    "    #--------------------------\n",
    "    #No alternate name is ' '\n",
    "    file_name = ''\n",
    "    #Check if accession id in file names, if not check related ids\n",
    "    if '2019-nCoV_'+identifier+'_variants.gff3' in all_names:\n",
    "        file_name = '2019-nCoV_'+identifier+'_variants.gff3'\n",
    "    # checking alternate names\n",
    "    elif meta.loc[identifier,'Related ID'] != ' ':\n",
    "        for alt_identifier in meta.loc[identifier,'Related ID'].replace(' ','').split(','):\n",
    "            if '2019-nCoV_'+alt_identifier+'_variants.gff3' in all_names:\n",
    "                file_name = '2019-nCoV_'+alt_identifier+'_variants.gff3'\n",
    "                break\n",
    "        #Added in case alternate names are also not found in gffs\n",
    "        if file_name == '':\n",
    "            missing.append(identifier)\n",
    "            continue\n",
    "    # If file name has not been updated, then there is no matching identifier, move to next index\n",
    "    elif file_name == '':\n",
    "        missing.append(identifier)\n",
    "        continue\n",
    "    #--------------------------\n",
    "    \n",
    "    #Filtering files with no variants\n",
    "    #--------------------------\n",
    "    with open(f'../gff3_cncb_restructured/{file_name}') as text_file:\n",
    "        lines = text_file.readlines()\n",
    "        counter = 0\n",
    "        for l in lines:\n",
    "            if '#' in l:\n",
    "                counter += 1\n",
    "    #Number of info lines should be less than total, if not then there are no mutations\n",
    "    #--------------------------\n",
    "    \n",
    "    #List to keep track of which files used already, save and import this in future to avoid redundant search\n",
    "    processed_identifiers.append(identifier)\n",
    "    \n",
    "    if counter<len(lines) and meta.loc[identifier,'Country'] in set(jh_data['Country_Region']): #country needs to be found in jh data for inf/fata rates\n",
    "        \n",
    "        gff = pd.read_csv(f'../gff3_cncb_restructured/{file_name}',sep='\\t',skiprows=counter,usecols=[1,3,4,8],names=column_names)\n",
    "        info_df = pd.DataFrame(gff['info'].str.split(';').values.tolist(),columns=[0,1,'Ref','Alt','Description']).drop([0,1],axis=1)\n",
    "        gff = gff.drop(['info'],axis=1)\n",
    "        gff['Country'] = [meta.loc[identifier,'Country']]*gff.shape[0]\n",
    "        temp_df = pd.concat([gff,info_df],axis=1)\n",
    "        \n",
    "        #Filtering alternate amino acid and reference for missense_variant and synonymous_variant\n",
    "        missenses_ref = temp_df.loc[temp_df['Description'].str.contains('missense_variant'),'Description'].str.split(',').str[1].str[-3]\n",
    "        synonymous_ref = temp_df.loc[temp_df['Description'].str.contains('synonymous_variant'),'Description'].str.split(',').str[1].str[-1]\n",
    "        temp_df['Ref_AA'] = pd.concat([missenses_ref,synonymous_ref])\n",
    "        temp_df['Alt_AA'] = temp_df.loc[temp_df['Description'].str.contains('missense_variant'),'Description'].str.split(',').str[1].str[-1]\n",
    "\n",
    "        missenses_str = temp_df.loc[temp_df['Description'].str.contains('missense_variant'),'Description'].str.split(',').str[1].str.split('.').str[-1]\n",
    "        synonymous_str = temp_df.loc[temp_df['Description'].str.contains('synonymous_variant'),'Description'].str.split(',').str[1].str.split('.').str[-1]\n",
    "        temp_df['AA'] = pd.concat([missenses_str,synonymous_str])\n",
    "        \n",
    "        temp_df.fillna('', inplace=True)\n",
    "        temp_df['descriptor'] = temp_df['start'].astype(str)+','+temp_df['end'].astype(str)+','+temp_df['Ref']+','+temp_df['Alt']+\\\n",
    "        ','+temp_df['Description'].str.split(',').str[0].str.split('=').str[1]+','+temp_df['variant type']+','+temp_df['Ref_AA']+','+temp_df['Alt_AA']+\\\n",
    "        ','+temp_df['AA']\n",
    "\n",
    "#         if any(temp_df['Ref'].str.contains('REF=A') & temp_df['Alt'].str.contains('ALT=GGTTC')):\n",
    "#             break\n",
    "        \n",
    "    \n",
    "        for var in temp_df['descriptor']:\n",
    "            #Dictionary for each sample date, each df then has its own variant descriptor (duplicates across dates)\n",
    "            if var not in aggregated_mutations[meta.loc[identifier,'Sample Collection Date']].keys():\n",
    "                aggregated_mutations[meta.loc[identifier,'Sample Collection Date']][var] = []\n",
    "\n",
    "            #Use country and sample collection date to parse jh_daily_ir/jh_daily_fr\n",
    "            #Country\n",
    "            aggregated_mutations[meta.loc[identifier,'Sample Collection Date']][var].append(temp_df.loc[0,'Country'])\n",
    "\n",
    "print(len(missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"jh_cncb_daily_dates_{dt.today().strftime('%m_%d_%y')}.pkl\",'wb') as file:\n",
    "    pickle.dump(aggregated_mutations,file,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(f\"jh_cncb_daily_dates_identifiers_{dt.today().strftime('%m_%d_%y')}.pkl\",'wb') as file:\n",
    "    pickle.dump(processed_identifiers,file,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need to combine sequences and their matched ir/fr for a sliding window, need to use same ir/fr for the whole window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* aggregated_muations dicitonary of dictionaries\n",
    "* level 1 - dates\n",
    "* level 2 - mutation names\n",
    "* for each mutation on level 2 contains countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = jh_daily_ir.columns[9::] #skip the first 9 (feb 1), arbitrarily since not too much data yet\n",
    "aggregated_mutations_window = {date:{} for date in date_range}\n",
    "\n",
    "#Gets cumlative country counts up to date\n",
    "\n",
    "#iterate over dates to get cumlative up to that date\n",
    "for date in tqdm(date_range):\n",
    "    #iterate over the preceding dates for the day that you are calculating\n",
    "    for window_date in pd.date_range(start=date_range[0],end=date):\n",
    "        for var in aggregated_mutations[window_date].keys():\n",
    "            if var not in aggregated_mutations_window[date].keys():\n",
    "                aggregated_mutations_window[date][var] = [[],[],[]]\n",
    "                \n",
    "            #Use country and date to get same ir/fr for that period\n",
    "            aggregated_mutations_window[date][var][0] += aggregated_mutations[window_date][var] #Appending country names\n",
    "            \n",
    "#             for country in aggregated_mutations[window_date][var]:\n",
    "#                 aggregated_mutations_window[date][var][1].append(jh_daily_ir.loc[country,date])\n",
    "#                 aggregated_mutations_window[date][var][2].append(jh_daily_fr.loc[country,date])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using cumlative country counts up to date\n",
    "* Counts cumlative countires for a variant and multiplies counts by the cumlative ir/fr\n",
    "* ir/fr is cumlative up to that date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looping over single days\n",
    "for date in tqdm(aggregated_mutations_window.keys()):\n",
    "    #Looping over single variants\n",
    "    for var in aggregated_mutations_window[date].keys():\n",
    "        #For each variant count countries, \n",
    "        for counted in Counter(aggregated_mutations_window[date][var][0]).items(): #counted is country and counts\n",
    "            aggregated_mutations_window[date][var][1].append(counted[1]*jh_daily_ir.loc[counted[0],date])\n",
    "            aggregated_mutations_window[date][var][2].append(counted[1]*jh_daily_fr.loc[counted[0],date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I dont think it finishes the last few dates, there are no sequences yet\n",
    "for date in tqdm(aggregated_mutations_window.keys()):\n",
    "    unique_muts = pd.DataFrame(list(aggregated_mutations_window[date].keys()),columns=['descriptor'])\n",
    "    unique_muts = pd.DataFrame.join(unique_muts,pd.DataFrame(unique_muts['descriptor'].str.split(',').to_list())) #assign columns with parsed descriptor\n",
    "    unique_muts.set_index('descriptor',inplace=True)\n",
    "    unique_muts[0] = pd.to_numeric(unique_muts[0])\n",
    "    unique_muts[1] = pd.to_numeric(unique_muts[1])\n",
    "    unique_muts.sort_values([0,1],inplace=True)\n",
    "    unique_muts.columns = ['Start','End','Ref','Alt','VEP','Variant Type','Ref_AA','Alt_AA','AA']\n",
    "\n",
    "    counts = {}\n",
    "    num_countries = {}\n",
    "    infection_rate = {}\n",
    "    fatality_rate = {}\n",
    "    counted_countries = {}\n",
    "\n",
    "    for desc in unique_muts.index:\n",
    "        counts[desc] = len(aggregated_mutations_window[date][desc][0])\n",
    "        num_countries[desc] = len(set(aggregated_mutations_window[date][desc][0]))\n",
    "        infection_rate[desc] = sum(aggregated_mutations_window[date][desc][1])/counts[desc]\n",
    "        #infection_rate[desc] = np.mean(aggregated_mutations_window[date][desc][1]) #since accumulated over countries before denomenator of mean is wrong\n",
    "        fatality_rate[desc] = sum(aggregated_mutations_window[date][desc][2])/counts[desc]\n",
    "        #fatality_rate[desc] = np.mean(aggregated_mutations_window[date][desc][2]) #since accumulated over countries before denomenator of mean is wrong\n",
    "        counted_countries[desc] = dict(Counter(aggregated_mutations_window[date][desc][0]))\n",
    "\n",
    "    unique_muts['counts'] = unique_muts.index.to_series().map(counts)\n",
    "    unique_muts['countries'] = unique_muts.index.to_series().map(num_countries)\n",
    "    unique_muts['infection_rate'] = unique_muts.index.to_series().map(infection_rate)\n",
    "    unique_muts['fatality_rate'] = unique_muts.index.to_series().map(fatality_rate)\n",
    "    unique_muts['counted_countries'] = unique_muts.index.to_series().map(counted_countries)\n",
    "    unique_muts.index = unique_muts.index.str.split(',').str[0:4].str.join('_')\n",
    "    unique_muts.sort_values('counts',ascending=False)\n",
    "    unique_muts.to_csv(f\"temporal_files/cumulative_daily1_7/{date.strftime('%m_%d_%y')}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3.8_misc]",
   "language": "python",
   "name": "conda-env-py3.8_misc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
